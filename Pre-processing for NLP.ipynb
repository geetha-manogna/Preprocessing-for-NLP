{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4bd7255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The choice was red, green, or blue. It didn't seem like an important choice when he was making it, but it was a choice nonetheless. Had he known the consequences at that time, he would likely have considered the choice a bit longer. In the end, he didn't and ended up choosing blue.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The choice was red, green, or blue. It didn't seem like an important choice when he was making it, but it was a choice nonetheless. Had he known the consequences at that time, he would likely have considered the choice a bit longer. In the end, he didn't and ended up choosing blue.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b584043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/geethamanognagoddu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d8d8ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The choice was red, green, or blue.',\n",
       " \"It didn't seem like an important choice when he was making it, but it was a choice nonetheless.\",\n",
       " 'Had he known the consequences at that time, he would likely have considered the choice a bit longer.',\n",
       " \"In the end, he didn't and ended up choosing blue.\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce224411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Had he known the consequences at that time, he would likely have considered the choice a bit longer.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "423f4d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Had he known the consequences at that time  he would likely have considered the choice a bit longer '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove punctuation\n",
    "import re\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", sentences[2]) \n",
    "text"
   ]
  },
  {
   "cell_type": "raw",
   "id": "edcf645d",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7e6fcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f313c10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Had', 'he', 'known', 'the', 'consequences', 'at', 'that', 'time', 'he', 'would', 'likely', 'have', 'considered', 'the', 'choice', 'a', 'bit', 'longer']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35db61cb",
   "metadata": {},
   "source": [
    "Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0785311e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/geethamanognagoddu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddf8c8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Had', 'known', 'consequences', 'time', 'would', 'likely', 'considered', 'choice', 'bit', 'longer']\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1787800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se', 'las', 'por', 'un', 'para', 'con', 'no', 'una', 'su', 'al', 'lo', 'como', 'más', 'pero', 'sus', 'le', 'ya', 'o', 'este', 'sí', 'porque', 'esta', 'entre', 'cuando', 'muy', 'sin', 'sobre', 'también', 'me', 'hasta', 'hay', 'donde', 'quien', 'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni', 'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto', 'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras', 'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada', 'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas', 'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus', 'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía', 'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya', 'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras', 'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas', 'estoy', 'estás', 'está', 'estamos', 'estáis', 'están', 'esté', 'estés', 'estemos', 'estéis', 'estén', 'estaré', 'estarás', 'estará', 'estaremos', 'estaréis', 'estarán', 'estaría', 'estarías', 'estaríamos', 'estaríais', 'estarían', 'estaba', 'estabas', 'estábamos', 'estabais', 'estaban', 'estuve', 'estuviste', 'estuvo', 'estuvimos', 'estuvisteis', 'estuvieron', 'estuviera', 'estuvieras', 'estuviéramos', 'estuvierais', 'estuvieran', 'estuviese', 'estuvieses', 'estuviésemos', 'estuvieseis', 'estuviesen', 'estando', 'estado', 'estada', 'estados', 'estadas', 'estad', 'he', 'has', 'ha', 'hemos', 'habéis', 'han', 'haya', 'hayas', 'hayamos', 'hayáis', 'hayan', 'habré', 'habrás', 'habrá', 'habremos', 'habréis', 'habrán', 'habría', 'habrías', 'habríamos', 'habríais', 'habrían', 'había', 'habías', 'habíamos', 'habíais', 'habían', 'hube', 'hubiste', 'hubo', 'hubimos', 'hubisteis', 'hubieron', 'hubiera', 'hubieras', 'hubiéramos', 'hubierais', 'hubieran', 'hubiese', 'hubieses', 'hubiésemos', 'hubieseis', 'hubiesen', 'habiendo', 'habido', 'habida', 'habidos', 'habidas', 'soy', 'eres', 'es', 'somos', 'sois', 'son', 'sea', 'seas', 'seamos', 'seáis', 'sean', 'seré', 'serás', 'será', 'seremos', 'seréis', 'serán', 'sería', 'serías', 'seríamos', 'seríais', 'serían', 'era', 'eras', 'éramos', 'erais', 'eran', 'fui', 'fuiste', 'fue', 'fuimos', 'fuisteis', 'fueron', 'fuera', 'fueras', 'fuéramos', 'fuerais', 'fueran', 'fuese', 'fueses', 'fuésemos', 'fueseis', 'fuesen', 'sintiendo', 'sentido', 'sentida', 'sentidos', 'sentidas', 'siente', 'sentid', 'tengo', 'tienes', 'tiene', 'tenemos', 'tenéis', 'tienen', 'tenga', 'tengas', 'tengamos', 'tengáis', 'tengan', 'tendré', 'tendrás', 'tendrá', 'tendremos', 'tendréis', 'tendrán', 'tendría', 'tendrías', 'tendríamos', 'tendríais', 'tendrían', 'tenía', 'tenías', 'teníamos', 'teníais', 'tenían', 'tuve', 'tuviste', 'tuvo', 'tuvimos', 'tuvisteis', 'tuvieron', 'tuviera', 'tuvieras', 'tuviéramos', 'tuvierais', 'tuvieran', 'tuviese', 'tuvieses', 'tuviésemos', 'tuvieseis', 'tuviesen', 'teniendo', 'tenido', 'tenida', 'tenidos', 'tenidas', 'tened']\n"
     ]
    }
   ],
   "source": [
    "# have a look at the stop words in nltk's corpus\n",
    "print(stopwords.words(\"spanish\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6763481c",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5fbcdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/geethamanognagoddu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/geethamanognagoddu/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet') # download for lemmatization\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07ebad9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['had', 'known', 'consequ', 'time', 'would', 'like', 'consid', 'choic', 'bit', 'longer']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08afe006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Had', 'known', 'consequence', 'time', 'would', 'likely', 'considered', 'choice', 'bit', 'longer']\n"
     ]
    }
   ],
   "source": [
    "#lemmatize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Reduce words to their root form\n",
    "lemmatized = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c62fd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming output: ['wait', 'wait', 'studi', 'studi', 'comput']\n",
      "Lemmatization output: ['wait', 'waiting', 'study', 'studying', 'computer']\n"
     ]
    }
   ],
   "source": [
    "# Another stemming and lemmatization example\n",
    "words2 = ['wait', 'waiting' , 'studies', 'studying', 'computers']\n",
    "\n",
    "# Stemming\n",
    "# Reduce words to their stems\n",
    "stemmed = [PorterStemmer().stem(w) for w in words2]\n",
    "print(\"Stemming output: {}\".format(stemmed))\n",
    "\n",
    "# Lemmatization\n",
    "# Reduce words to their root form\n",
    "lemmatized = [WordNetLemmatizer().lemmatize(w) for w in words2]\n",
    "print(\"Lemmatization output: {}\".format(lemmatized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b33341",
   "metadata": {},
   "source": [
    "Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1b1af98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/geethamanognagoddu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/geethamanognagoddu/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46d477f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b97d022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Had', 'NNP'),\n",
       " ('known', 'VBN'),\n",
       " ('consequences', 'NNS'),\n",
       " ('time', 'NN'),\n",
       " ('would', 'MD'),\n",
       " ('likely', 'RB'),\n",
       " ('considered', 'VBN'),\n",
       " ('choice', 'NN'),\n",
       " ('bit', 'NN'),\n",
       " ('longer', 'RBR')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tag each word with part of speech\n",
    "pos_tag(words)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b7e9349",
   "metadata": {},
   "source": [
    "Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8caf70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/geethamanognagoddu/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fde093b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Had/VBD\n",
      "  he/PRP\n",
      "  known/VB\n",
      "  the/DT\n",
      "  consequences/NNS\n",
      "  at/IN\n",
      "  that/DT\n",
      "  time/NN\n",
      "  ,/,\n",
      "  he/PRP\n",
      "  would/MD\n",
      "  likely/RB\n",
      "  have/VB\n",
      "  considered/VBN\n",
      "  the/DT\n",
      "  choice/NN\n",
      "  a/DT\n",
      "  bit/NN\n",
      "  longer/RBR\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "ner_tree = ne_chunk(pos_tag(word_tokenize(sentences[2])))\n",
    "print(ner_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acb68eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  choice/NN\n",
      "  was/VBD\n",
      "  red/JJ\n",
      "  ,/,\n",
      "  green/JJ\n",
      "  ,/,\n",
      "  or/CC\n",
      "  blue/NN\n",
      "  ./.\n",
      "  It/PRP\n",
      "  did/VBD\n",
      "  n't/RB\n",
      "  seem/VB\n",
      "  like/IN\n",
      "  an/DT\n",
      "  important/JJ\n",
      "  choice/NN\n",
      "  when/WRB\n",
      "  he/PRP\n",
      "  was/VBD\n",
      "  making/VBG\n",
      "  it/PRP\n",
      "  ,/,\n",
      "  but/CC\n",
      "  it/PRP\n",
      "  was/VBD\n",
      "  a/DT\n",
      "  choice/NN\n",
      "  nonetheless/RB\n",
      "  ./.\n",
      "  Had/VBD\n",
      "  he/PRP\n",
      "  known/VB\n",
      "  the/DT\n",
      "  consequences/NNS\n",
      "  at/IN\n",
      "  that/DT\n",
      "  time/NN\n",
      "  ,/,\n",
      "  he/PRP\n",
      "  would/MD\n",
      "  likely/RB\n",
      "  have/VB\n",
      "  considered/VBN\n",
      "  the/DT\n",
      "  choice/NN\n",
      "  a/DT\n",
      "  bit/NN\n",
      "  longer/RB\n",
      "  ./.\n",
      "  In/IN\n",
      "  the/DT\n",
      "  end/NN\n",
      "  ,/,\n",
      "  he/PRP\n",
      "  did/VBD\n",
      "  n't/RB\n",
      "  and/CC\n",
      "  ended/VBD\n",
      "  up/RP\n",
      "  choosing/VBG\n",
      "  blue/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "text = \"The choice was red, green, or blue. It didn't seem like an important choice when he was making it, but it was a choice nonetheless. Had he known the consequences at that time, he would likely have considered the choice a bit longer. In the end, he didn't and ended up choosing blue.\"\n",
    "\n",
    "ner_tree = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "print(ner_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50969443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Twitter/NNP)\n",
      "  (ORGANIZATION CEO/NNP Elon/NNP Musk/NNP)\n",
      "  arrived/VBD\n",
      "  at/IN\n",
      "  the/DT\n",
      "  (FACILITY Staples/NNP Center/NNP)\n",
      "  in/IN\n",
      "  (GPE Los/NNP Angeles/NNP)\n",
      "  ,/,\n",
      "  (GPE California/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "text = \"Twitter CEO Elon Musk arrived at the Staples Center in Los Angeles, California. \"\n",
    "ner_tree = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "print(ner_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f02e2aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce3af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d8f25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
